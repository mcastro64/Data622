# Week 3
Describe the main difference between Linear and Quadratic Discriminant Analysis. 

Linear and Quadratic Discriminant Analysis are classification methods that assume that observations within each class are drawn from a multivariate Guassian distribution. The main difference is that LDA assumes a shared covariance matrix among all K classes while QDA assumes that each class has its own coveriance vectors. Under this assumption, LDA uses a linear function of x in the Bayers classfier and its decision rule depends on x only through a linear combination of its elemenets. QDA uses a quadratic function of x. LDA is therefore a less flexible classifier than QDA and its lower variance can lead to prediction perfomance. However, if the assumption that the K classes share a covariance matrix is off, LDA can suffer from high bias. LDA tends to perform better than QDA when there are relatively few training observations and reducing variance is important, while QDA is recommended for large training sets or if the assumption of a shared coveriance vectors is incorrect.

How does k-Nearest Neighbors (kNN) classify data, and what role do distance metrics play in this process?

K-Nearest Neighbors classifies data by comparing the classification of "K" number of data points closest/nearest in distance – usually defined through its Eucledean distance – from each observation being evaluated. Each evaluated observation is assigned a classification value based on the majority class of its nearest neighbors. For example, if K = 3, we would take the three nearest points (A, B, C) to our observation and count how many of these observations have a class value of 0 and a class value of 1. If A is class 0 and B and C are class 1, we would classify our observation as 1. In the event that K is even and there is a tie, KNN assigns a classification at random. Therefore, it is better to use an odd value for K. As a rule of thumb, K is often the square root of the number of observations in our dataset. 

What are the strengths and weaknesses of kNN compared to discriminant analysis (LDA & QDA)?

A major strength of KNN is its simplicity and straightforward, making it easy to exmplain its basic concept. Additionally, KNN is non-parametric and makes no assumptions about feature distributions allowing it create non-linear decision boundaries. By comparison, LDA and QDA are more assume a Gaussian or normal distribution and a linear or near/linear/quadratic decision boundary; violating these assumptions could lead to poor accuracy or overfitting. As a lazy learner, KNN needs a fair amount of data and suffers from high computational cost needs to store its data making it comparitively slow, particularly in large,  dataset as it needs to compute all distances between points; KNN can also be more sensitive to noisy data and can perform poorly with high-dimensional data where distances become less meaniningful. By comparison, LDA/QDA are much more perfomant as they rely on modeling that "learn" how to predict based on past observations and data. LDA/QDA also works better with small datasets. 


Week #4 
What assumptions are made by the Naive Bayes classifier?

Describe how Naive Bayes handles continuous vs. categorical features?

Compare Naive Bayes to k-Nearest Neighbors and Discriminant Analysis in terms of performance and applicability
